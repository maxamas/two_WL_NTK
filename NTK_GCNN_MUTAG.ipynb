{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CLlHJB7YguRG"},"outputs":[],"source":["!pip install -q --upgrade pip\n","!pip install -q --upgrade jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","!pip install -q git+https://www.github.com/google/neural-tangents\n","!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n","!pip install pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.13.0+cpu.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTYjV6GAg5XR"},"outputs":[],"source":["!pip uninstall jaxlib\n","!pip install jaxlib==0.4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NaGfUJhg8NM"},"outputs":[],"source":["import neural_tangents as nt\n","from neural_tangents import stax\n","from jax import numpy as np\n","from jax import random\n","import jax\n","\n","\n","def diag(x, batched=True):\n","  \"\"\"\n","  Arange a 2-dim arrary into a 3-dim array.\n","  Where the 3-dim array has in the channel\n","  dimension diagonal matricies filled\n","  with the values from the 2-dim input.\n","  e.g \n","  diag(np.array[[1,3],[3,4], [5,6]])\n","  = [ [[1,0], [0,3]], [[3,0], [0,4]], [[5,0], [0,6]]]\n","  \"\"\"\n","  if batched:\n","    out = np.zeros((x.shape[0], x.shape[1], x.shape[1]))\n","    for i in range(0, x.shape[1]):\n","      out = out.at[:,i,i].set(x[:,i])\n","  else:\n","    out = np.zeros((x.shape[0], x.shape[0]))\n","    out = out.at[np.diag_indices(out.shape[0])].set(x)\n","  return out\n","\n","\n","def grap_conv_pattern(A, batched=True):\n","  A_tilde = A + np.identity(A.shape[1])\n","  A_tilde = A_tilde.at[A_tilde == 2].set(1)\n","  if batched:\n","    D_tilde = np.sum(A_tilde, axis=2)\n","  else:\n","    D_tilde = np.sum(A_tilde, axis=1)\n","  D_tilde = 1/np.sqrt(D_tilde)\n","  D_tilde = diag(D_tilde, batched)\n","  return D_tilde @ A_tilde @ D_tilde\n","\n","def expand_pattern_at_channels_dim(pattern_in, nr_channels):\n","  \"\"\"\n","  Expand a (batched) two dimensional pattern \n","  into a three dimensional pattern. The size of the added \n","  dimension is determined by nr_channels.\n","  The channe\n","  \"\"\"\n","  pattern_out = np.zeros((pattern_in.shape[0],\n","                          pattern_in.shape[1], nr_channels, \n","                          pattern_in.shape[1], nr_channels))\n","  for k in range(pattern_in.shape[0]):\n","    for i in range(pattern_in.shape[1]):\n","      for j in range(pattern_in.shape[2]):\n","        pattern_out = pattern_out.at[k,i,:,j,:].set(np.full((nr_channels,nr_channels), pattern_in[k,i,j]))\n","  return pattern_out\n","\n","def expand_pattern_at_channels_dim(pattern_in, nr_channels, batched=True):\n","  \"\"\"\n","  Expand a (batched) two dimensional pattern \n","  into a three dimensional pattern. The size of the added \n","  dimension is determined by nr_channels.\n","  The channe\n","  \"\"\"\n","\n","  if batched:\n","      out = np.zeros((pattern_in.shape[0],\n","                          pattern_in.shape[1], nr_channels, \n","                          pattern_in.shape[1], nr_channels))\n","      for k in range(pattern_in.shape[0]):\n","        for i in range(nr_channels):\n","          out = out.at[k,:,i,:,i].set(pattern_in[k,:])\n","  else:\n","    out = np.zeros((pattern_in.shape[1], nr_channels, \n","                    pattern_in.shape[1], nr_channels))\n","    for i in range(nr_channels):\n","      out = out.at[:,i,:,i].set(pattern_in)\n","  return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lf6luQichEPx"},"outputs":[],"source":["# Grap Convolution Mutag Datataset Example\n","\n","from torch_geometric.datasets import TUDataset\n","from torch_geometric.loader import DataLoader\n","\n","def to_dense(node_list, size):\n","  \"\"\"\n","  Naive implementation, to get a\n","  adjacency matrix from a node list.\n","  Node list 2xn -> adjacency matrix nxn\n","  \"\"\"\n","  A = np.zeros((size, size))\n","  node_list = data.edge_index.tolist()\n","  for i,j in zip(node_list[0], node_list[1]):\n","    A = A.at[i,j].set(1)\n","  return A\n","\n","def zero_append(a, shape):\n","  \"\"\"\n","  Add zero columns and rows to the array \n","  a, to make it of shape size x size.\n","  \"\"\"\n","  out = np.zeros((shape[0],shape[1]))\n","  out = out.at[:a.shape[0],:a.shape[1]].set(a)\n","  return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"auKPtP0HhH2T"},"outputs":[],"source":["# make lists of dense adjacency matrix, node feature array and y\n","\n","dataset = TUDataset(root=\"Masterarbeit\", name=\"MUTAG\")\n","\n","As = list()\n","graps_edge_features = list()\n","ys = list()\n","\n","for data in dataset:\n","  As.append(to_dense(data.edge_index, len(data.x)))\n","  graps_edge_features.append(np.array(data.x))\n","  ys.append(np.array(data.y))\n","\n","# unify the sizes for the nn input\n","max_nodes = len(max(graps_edge_features, key=lambda x: len(x)))\n","graps_edge_features = [zero_append(ef, (max_nodes, ef.shape[1])) \n","for ef in graps_edge_features]\n","As = [zero_append(a, (max_nodes, max_nodes)) for a in As]\n","\n","# calcualte the graph convolution pattern for each graph\n","patterns = list()\n","for A in As:\n","  p = grap_conv_pattern(A, False)\n","  patterns.append(expand_pattern_at_channels_dim(p, 7, False))\n","\n","graps_edge_features_2 = np.array(graps_edge_features)\n","graps_edge_features_2 = np.expand_dims(graps_edge_features_2, 3)\n","patterns = np.array(patterns)\n","\n","\n","# define a grap convolution network and calculate the kernel matrix for it\n","init_fn, apply_fn, kernel_fn = stax.serial(\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.Aggregate(aggregate_axis=(1, 2), batch_axis=0, channel_axis=3),\n","    stax.Conv(100, (1,1)), stax.Relu(),\n","    stax.GlobalSumPool(), \n",")\n","\n","size = 37\n","for i in range(5):\n","  for j in range(5):\n","    x1 = graps_edge_features_2[i*size:(1+i)*size,:]\n","    x2 = graps_edge_features_2[j*size:(1+j)*size,:]\n","    p1 = patterns[i*size:(1+i)*size,:]\n","    p2 = patterns[j*size:(1+j)*size,:]\n","\n","    kernel_matrix = kernel_fn(x1, x2, 'nngp', pattern=(p1, p2))\n","    np.save(f\"kernel_matrix_{(1+i)*size}_{(1+j)*size}\", kernel_matrix)\n","np.save(f\"ys\", ys)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
