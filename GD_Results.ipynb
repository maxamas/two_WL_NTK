{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCAAY5QjYi1d0liy0L0eBz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install -q --upgrade pip\n","!pip install -U jaxlib==0.4.4+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","!pip install -q --upgrade jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","!pip install -q git+https://www.github.com/google/neural-tangents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HfI3QAUt5x0d","executionInfo":{"status":"ok","timestamp":1678269479492,"user_tz":-60,"elapsed":44854,"user":{"displayName":"Max Hahn","userId":"05735514096494257430"}},"outputId":"f53150da-bdf7-4c82-807e-27d5ad8a26a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n","Collecting jaxlib==0.4.4+cuda11.cudnn82\n","  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.4%2Bcuda11.cudnn82-cp38-cp38-manylinux2014_x86_64.whl (154.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.4.4+cuda11.cudnn82) (1.24.2)\n","Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jaxlib==0.4.4+cuda11.cudnn82) (1.10.1)\n","Installing collected packages: jaxlib\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.4.2+cuda11.cudnn82\n","    Uninstalling jaxlib-0.4.2+cuda11.cudnn82:\n","      Successfully uninstalled jaxlib-0.4.2+cuda11.cudnn82\n","Successfully installed jaxlib-0.4.4+cuda11.cudnn82\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["jaxlib"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: jax 0.4.5 does not provide the extra 'cuda11_cudnn805'\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cpu\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.24.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.pyg.org/whl/torch-1.13.0+cpu.html\n","Requirement already satisfied: pyg-lib in /usr/local/lib/python3.8/dist-packages (0.1.0+pt113cpu)\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.8/dist-packages (2.1.0+pt113cpu)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.8/dist-packages (0.6.16+pt113cpu)\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.8/dist-packages (1.6.0+pt113cpu)\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.8/dist-packages (1.2.1+pt113cpu)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.8/dist-packages (2.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.10.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.2.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (5.9.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.25.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.26.14)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks/Masterarbeit/two_WL_NTK"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZea_r_L92yp","executionInfo":{"status":"ok","timestamp":1678269840944,"user_tz":-60,"elapsed":2784,"user":{"displayName":"Max Hahn","userId":"05735514096494257430"}},"outputId":"b80a42b0-0f39-40fb-bdc9-86942f5e5053"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Masterarbeit/two_WL_NTK\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/Masterarbeit/two_WL_NTK"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HWrXKO9e8Ca_","executionInfo":{"status":"ok","timestamp":1678297747460,"user_tz":-60,"elapsed":1562,"user":{"displayName":"Max Hahn","userId":"05735514096494257430"}},"outputId":"ed7549ba-0b36-4f32-e0da-3e9bc51454f8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Masterarbeit/two_WL_NTK\n"]}]},{"cell_type":"code","source":["from network_config import get_network_configuration\n","from train import cross_validate, cross_entropy, save_GD_raw_results, save_core_results\n","\n","from jax import numpy as jnp\n","import matplotlib.pyplot as plt\n","from jax import jit, grad, vmap\n","\n","import numpy as np\n","import os\n","import pandas as pd"],"metadata":{"id":"FP-qC3406DdK","executionInfo":{"status":"ok","timestamp":1678297766072,"user_tz":-60,"elapsed":8167,"user":{"displayName":"Max Hahn","userId":"05735514096494257430"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# MUTAG 2 WL\n","\n","utc_time = pd.Timestamp.utcnow()\n","dataset_name = \"MUTAG\"\n","training_method = \"Gradient_Descent\"\n","nn_type = \"twl\"\n","\n","preprocessed_base_path = f\"/content/drive/MyDrive/MasterarbeitData/Preprocessed/{dataset_name}\"\n","repo_path = \"/content/drive/MyDrive/Colab Notebooks/Masterarbeit/two_WL_NTK\"\n","\n","cv_folds = 10\n","learning_rate =  0.0005\n","epochs = 200\n","\n","# load preprocessed data\n","Y = jnp.load(preprocessed_base_path + f\"/ys.npy\")\n","graphs_edge_features = jnp.load(preprocessed_base_path + f\"/graphs_edge_features.npy\")\n","graph_two_wl_pattern = jnp.load(preprocessed_base_path + f\"/two_wl_pattern_radius_1.npy\")\n","\n","# load the network configuration\n","init_fn, apply_fn, kernel_fn = get_network_configuration(dataset_name, nn_type, training_method)\n","\n","# loss function\n","loss = jit(cross_entropy)\n","grad_loss = jit(grad(lambda params, x, y, pattern: loss(y, apply_fn(params, x, pattern=pattern))))\n","\n","# cross validation training, to report validation acc mean and std\n","train_results = cross_validate(\n","    graphs_edge_features,\n","    Y,\n","    graph_two_wl_pattern,\n","    cv_folds,\n","    init_fn,\n","    apply_fn,\n","    learning_rate,\n","    epochs,\n","    nn_type,\n","    loss,\n","    grad_loss)\n","\n","\n","save_GD_raw_results(\n","    dataset_name, training_method, nn_type, epochs, utc_time.timestamp(), repo_path, cv_folds, train_results\n",")\n","save_core_results(np.array([np.array(i) for i in train_results[3]]), utc_time, dataset_name, training_method, nn_type, cv_folds, repo_path)"],"metadata":{"id":"NzsitUUH6GI8","executionInfo":{"status":"ok","timestamp":1678298326686,"user_tz":-60,"elapsed":503900,"user":{"displayName":"Max Hahn","userId":"05735514096494257430"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf1b1258-93b6-4a09-c915-ce1c58222e2d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Start CV fold: 0\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 110.92 | val loss: 134.46 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 80.77 | val loss: 98.03 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 50.78 | val loss: 61.79 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 16.91 | val loss: 20.79 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 5.91 | val loss: 3.49 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 5.39 | val loss: 3.18 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 1.96 | val loss: 2.57 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 1.40 | val loss: 0.82 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.98 | val loss: 1.25 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 0.66 | val loss: 0.56 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.68 | val loss: 0.53 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.63 | val loss: 0.60 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.63 | val loss: 0.62 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.62 | val loss: 0.57 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.61 | val loss: 0.55 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.60 | val loss: 0.54 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.60 | val loss: 0.54 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.59 | val loss: 0.53 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.58 | val loss: 0.52 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.58 | val loss: 0.51 | train acc: 0.6491 | val acc: 0.7895\n","Start CV fold: 1\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 113.94 | val loss: 107.34 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 82.97 | val loss: 78.30 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 52.17 | val loss: 49.40 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 17.36 | val loss: 16.63 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 5.71 | val loss: 6.17 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 5.67 | val loss: 6.14 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 1.00 | val loss: 1.08 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 1.21 | val loss: 1.23 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.94 | val loss: 1.01 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 0.72 | val loss: 0.73 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.65 | val loss: 0.66 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.63 | val loss: 0.66 | train acc: 0.6608 | val acc: 0.6316\n","\t train loss: 0.61 | val loss: 0.64 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.61 | val loss: 0.63 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.60 | val loss: 0.62 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.59 | val loss: 0.61 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.59 | val loss: 0.61 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.58 | val loss: 0.60 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.57 | val loss: 0.59 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.57 | val loss: 0.59 | train acc: 0.6667 | val acc: 0.6316\n","Start CV fold: 2\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 112.94 | val loss: 116.31 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 82.25 | val loss: 84.84 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 51.74 | val loss: 53.52 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 17.21 | val loss: 18.00 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 5.77 | val loss: 5.25 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 5.58 | val loss: 5.08 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 1.35 | val loss: 1.55 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 1.29 | val loss: 1.12 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.98 | val loss: 1.12 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 0.69 | val loss: 0.64 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.66 | val loss: 0.62 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.63 | val loss: 0.65 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.62 | val loss: 0.64 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.61 | val loss: 0.61 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.60 | val loss: 0.60 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.59 | val loss: 0.60 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.59 | val loss: 0.60 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.58 | val loss: 0.59 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.57 | val loss: 0.59 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.57 | val loss: 0.58 | train acc: 0.6608 | val acc: 0.6842\n","Start CV fold: 3\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 114.95 | val loss: 98.48 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 83.70 | val loss: 71.83 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 52.61 | val loss: 45.28 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 17.52 | val loss: 15.23 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 5.63 | val loss: 7.12 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 5.74 | val loss: 7.27 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.75 | val loss: 0.78 | train acc: 0.1696 | val acc: 0.3684\n","\t train loss: 1.11 | val loss: 1.34 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.87 | val loss: 0.87 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 0.74 | val loss: 0.86 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.63 | val loss: 0.70 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.63 | val loss: 0.67 | train acc: 0.6784 | val acc: 0.6316\n","\t train loss: 0.61 | val loss: 0.66 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.60 | val loss: 0.66 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.60 | val loss: 0.65 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.59 | val loss: 0.64 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.58 | val loss: 0.63 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.58 | val loss: 0.63 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.57 | val loss: 0.62 | train acc: 0.6725 | val acc: 0.5789\n","\t train loss: 0.57 | val loss: 0.62 | train acc: 0.6725 | val acc: 0.5789\n","Start CV fold: 4\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 111.97 | val loss: 125.47 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 81.53 | val loss: 91.48 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 51.27 | val loss: 57.68 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 17.05 | val loss: 19.38 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 5.83 | val loss: 4.35 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 5.49 | val loss: 4.10 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 1.65 | val loss: 2.02 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 1.34 | val loss: 0.95 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.98 | val loss: 1.19 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 0.67 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.66 | val loss: 0.56 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.62 | val loss: 0.62 | train acc: 0.6608 | val acc: 0.7368\n","\t train loss: 0.62 | val loss: 0.62 | train acc: 0.6901 | val acc: 0.7368\n","\t train loss: 0.60 | val loss: 0.59 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.60 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.59 | val loss: 0.57 | train acc: 0.6667 | val acc: 0.7368\n","\t train loss: 0.58 | val loss: 0.58 | train acc: 0.6901 | val acc: 0.7895\n","\t train loss: 0.58 | val loss: 0.58 | train acc: 0.6959 | val acc: 0.7895\n","\t train loss: 0.57 | val loss: 0.58 | train acc: 0.6959 | val acc: 0.7895\n","\t train loss: 0.57 | val loss: 0.58 | train acc: 0.6959 | val acc: 0.7895\n","Start CV fold: 5\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 113.96 | val loss: 107.39 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 82.98 | val loss: 78.32 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 52.17 | val loss: 49.42 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 17.36 | val loss: 16.64 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 5.70 | val loss: 6.16 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 5.67 | val loss: 6.14 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.98 | val loss: 1.08 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 1.19 | val loss: 1.21 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.93 | val loss: 1.03 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 0.71 | val loss: 0.72 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.64 | val loss: 0.65 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.62 | val loss: 0.68 | train acc: 0.6784 | val acc: 0.6316\n","\t train loss: 0.61 | val loss: 0.65 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.60 | val loss: 0.63 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.59 | val loss: 0.63 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.59 | val loss: 0.62 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.58 | val loss: 0.62 | train acc: 0.6842 | val acc: 0.6316\n","\t train loss: 0.57 | val loss: 0.62 | train acc: 0.6842 | val acc: 0.7368\n","\t train loss: 0.56 | val loss: 0.62 | train acc: 0.6901 | val acc: 0.8947\n","\t train loss: 0.56 | val loss: 0.62 | train acc: 0.6959 | val acc: 0.9474\n","Start CV fold: 6\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 117.96 | val loss: 71.54 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 85.90 | val loss: 52.17 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 54.01 | val loss: 32.88 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 17.97 | val loss: 11.01 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 5.41 | val loss: 9.88 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 5.92 | val loss: 10.84 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.97 | val loss: 1.59 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.78 | val loss: 1.20 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.65 | val loss: 0.69 | train acc: 0.6842 | val acc: 0.4737\n","\t train loss: 0.76 | val loss: 1.17 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.62 | val loss: 0.69 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.62 | val loss: 0.68 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.60 | val loss: 0.75 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.59 | val loss: 0.74 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.59 | val loss: 0.71 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.58 | val loss: 0.70 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.57 | val loss: 0.70 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.57 | val loss: 0.70 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.56 | val loss: 0.70 | train acc: 0.6901 | val acc: 0.4211\n","\t train loss: 0.56 | val loss: 0.70 | train acc: 0.6901 | val acc: 0.4211\n","Start CV fold: 7\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 110.92 | val loss: 134.39 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 80.78 | val loss: 97.99 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 50.80 | val loss: 61.78 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 16.92 | val loss: 20.76 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 5.91 | val loss: 3.48 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 5.40 | val loss: 3.18 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 1.96 | val loss: 2.54 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 1.40 | val loss: 0.80 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.97 | val loss: 1.22 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 0.66 | val loss: 0.54 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.68 | val loss: 0.50 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.63 | val loss: 0.57 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.63 | val loss: 0.60 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.62 | val loss: 0.55 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.61 | val loss: 0.53 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.61 | val loss: 0.52 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.60 | val loss: 0.52 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.59 | val loss: 0.51 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.59 | val loss: 0.50 | train acc: 0.6491 | val acc: 0.7895\n","\t train loss: 0.58 | val loss: 0.49 | train acc: 0.6491 | val acc: 0.7895\n","Start CV fold: 8\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 111.95 | val loss: 125.21 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 81.53 | val loss: 91.33 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 51.28 | val loss: 57.59 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 17.05 | val loss: 19.33 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 5.84 | val loss: 4.38 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 5.49 | val loss: 4.12 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 1.69 | val loss: 2.04 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 1.35 | val loss: 0.99 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.99 | val loss: 1.19 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 0.67 | val loss: 0.58 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.67 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.63 | val loss: 0.62 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.62 | val loss: 0.63 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.61 | val loss: 0.59 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.61 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.60 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.59 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.59 | val loss: 0.57 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.58 | val loss: 0.56 | train acc: 0.6550 | val acc: 0.7368\n","\t train loss: 0.57 | val loss: 0.55 | train acc: 0.6550 | val acc: 0.7368\n","Start CV fold: 9\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 113.96 | val loss: 107.42 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 83.31 | val loss: 78.65 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 54.19 | val loss: 51.28 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 21.02 | val loss: 20.03 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 4.76 | val loss: 5.11 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 6.36 | val loss: 6.88 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 1.07 | val loss: 1.07 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.74 | val loss: 0.73 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.66 | val loss: 0.68 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.79 | val loss: 0.79 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.65 | val loss: 0.67 | train acc: 0.6550 | val acc: 0.5789\n","\t train loss: 0.63 | val loss: 0.66 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.61 | val loss: 0.63 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.60 | val loss: 0.63 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.59 | val loss: 0.63 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.59 | val loss: 0.62 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.58 | val loss: 0.62 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.57 | val loss: 0.61 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.57 | val loss: 0.61 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.56 | val loss: 0.61 | train acc: 0.6667 | val acc: 0.6316\n","mean: 0.77894735\n","std: 0.090550795\n","min: 0.6842105388641357\n","q_25: 0.6842105388641357\n","q_50: 0.7894737124443054\n","q_75: 0.8289473801851273\n","max: 0.9473684430122375\n"]}]},{"cell_type":"code","source":["# MUTAG GCN\n","\n","utc_time = pd.Timestamp.utcnow()\n","dataset_name = \"MUTAG\"\n","training_method = \"Gradient_Descent\"\n","nn_type = \"gcn\"\n","\n","preprocessed_base_path = f\"/content/drive/MyDrive/MasterarbeitData/Preprocessed/{dataset_name}\"\n","repo_path = \"/content/drive/MyDrive/Colab Notebooks/Masterarbeit/two_WL_NTK\"\n","\n","cv_folds = 10\n","learning_rate =  0.0001\n","epochs = 200\n","\n","# load preprocessed data\n","Y = jnp.load(preprocessed_base_path + f\"/ys.npy\")\n","graps_node_features = np.load(preprocessed_base_path + f\"/graps_node_features.npy\")\n","graph_conv_pattern = np.load(preprocessed_base_path + f\"/graph_conv_pattern.npy\")\n","graps_node_features = np.swapaxes(graps_node_features, 2,3)\n","\n","# load the network configuration\n","init_fn, apply_fn, kernel_fn = get_network_configuration(dataset_name, nn_type, training_method)\n","\n","# loss function\n","loss = jit(cross_entropy)\n","grad_loss = jit(grad(lambda params, x, y, pattern: loss(y, apply_fn(params, x, pattern=pattern))))\n","\n","# cross validation training, to report validation acc mean and std\n","train_results = cross_validate(\n","    graps_node_features,\n","    Y,\n","    graph_conv_pattern,\n","    cv_folds,\n","    init_fn,\n","    apply_fn,\n","    learning_rate,\n","    epochs,\n","    nn_type,\n","    loss,\n","    grad_loss)\n","\n","\n","save_GD_raw_results(\n","    dataset_name, training_method, nn_type, epochs, utc_time.timestamp(), repo_path, cv_folds, train_results\n",")\n","save_core_results(np.array([np.array(i) for i in train_results[3]]), utc_time, dataset_name, training_method, nn_type, cv_folds, repo_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8m9QE6MM_ps","executionInfo":{"status":"ok","timestamp":1678298664322,"user_tz":-60,"elapsed":337686,"user":{"displayName":"Max Hahn","userId":"05735514096494257430"}},"outputId":"d7b024af-cd99-483e-914c-716e1fb3a4a8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Start CV fold: 0\n","nb train samples: 171 | nb val samples: 19\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/neural_tangents/_src/stax/linear.py:439: UserWarning: Negative indices in `pattern` are considered as padding (i.e. ignored), unlike typical numpy negative indexing.\n","  warnings.warn('Negative indices in `pattern` are considered as padding '\n"]},{"output_type":"stream","name":"stdout","text":["\t train loss: 34.90 | val loss: 34.81 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 31.32 | val loss: 31.17 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 27.75 | val loss: 27.54 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 24.18 | val loss: 23.93 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 20.62 | val loss: 20.35 | train acc: 0.3509 | val acc: 0.2632\n","\t train loss: 17.05 | val loss: 16.78 | train acc: 0.3509 | val acc: 0.2632\n","\t train loss: 13.49 | val loss: 13.25 | train acc: 0.3567 | val acc: 0.2632\n","\t train loss: 9.98 | val loss: 9.78 | train acc: 0.3801 | val acc: 0.3684\n","\t train loss: 6.60 | val loss: 6.42 | train acc: 0.4211 | val acc: 0.3684\n","\t train loss: 3.48 | val loss: 3.26 | train acc: 0.4737 | val acc: 0.4211\n","\t train loss: 1.07 | val loss: 0.88 | train acc: 0.6608 | val acc: 0.6842\n","\t train loss: 0.67 | val loss: 0.50 | train acc: 0.7544 | val acc: 0.7895\n","\t train loss: 0.62 | val loss: 0.44 | train acc: 0.7836 | val acc: 0.8421\n","\t train loss: 0.57 | val loss: 0.37 | train acc: 0.7602 | val acc: 0.7368\n","\t train loss: 0.56 | val loss: 0.36 | train acc: 0.7602 | val acc: 0.7368\n","\t train loss: 0.56 | val loss: 0.36 | train acc: 0.7778 | val acc: 0.9474\n","\t train loss: 0.55 | val loss: 0.36 | train acc: 0.7778 | val acc: 0.8947\n","\t train loss: 0.55 | val loss: 0.36 | train acc: 0.7485 | val acc: 0.8421\n","\t train loss: 0.55 | val loss: 0.36 | train acc: 0.7544 | val acc: 0.8421\n","\t train loss: 0.55 | val loss: 0.36 | train acc: 0.7661 | val acc: 0.8421\n","Start CV fold: 1\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 35.21 | val loss: 31.97 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 31.59 | val loss: 28.71 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 27.98 | val loss: 25.44 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 24.38 | val loss: 22.18 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 20.78 | val loss: 18.92 | train acc: 0.3392 | val acc: 0.3684\n","\t train loss: 17.18 | val loss: 15.66 | train acc: 0.3392 | val acc: 0.3684\n","\t train loss: 13.59 | val loss: 12.41 | train acc: 0.3450 | val acc: 0.3684\n","\t train loss: 10.06 | val loss: 9.21 | train acc: 0.3743 | val acc: 0.4211\n","\t train loss: 6.66 | val loss: 6.09 | train acc: 0.4152 | val acc: 0.4211\n","\t train loss: 3.50 | val loss: 3.20 | train acc: 0.4678 | val acc: 0.4211\n","\t train loss: 1.06 | val loss: 1.09 | train acc: 0.6667 | val acc: 0.5789\n","\t train loss: 0.63 | val loss: 0.98 | train acc: 0.7602 | val acc: 0.7368\n","\t train loss: 0.57 | val loss: 0.93 | train acc: 0.7953 | val acc: 0.7368\n","\t train loss: 0.52 | val loss: 0.80 | train acc: 0.7661 | val acc: 0.6842\n","\t train loss: 0.51 | val loss: 0.80 | train acc: 0.7602 | val acc: 0.6842\n","\t train loss: 0.51 | val loss: 0.83 | train acc: 0.8070 | val acc: 0.6842\n","\t train loss: 0.50 | val loss: 0.82 | train acc: 0.8129 | val acc: 0.6842\n","\t train loss: 0.50 | val loss: 0.81 | train acc: 0.7661 | val acc: 0.6842\n","\t train loss: 0.50 | val loss: 0.82 | train acc: 0.8070 | val acc: 0.6842\n","\t train loss: 0.50 | val loss: 0.82 | train acc: 0.8070 | val acc: 0.6316\n","Start CV fold: 2\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 34.86 | val loss: 35.11 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 31.28 | val loss: 31.51 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 27.71 | val loss: 27.91 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 24.14 | val loss: 24.32 | train acc: 0.3392 | val acc: 0.3158\n","\t train loss: 20.58 | val loss: 20.73 | train acc: 0.3450 | val acc: 0.3158\n","\t train loss: 17.01 | val loss: 17.13 | train acc: 0.3450 | val acc: 0.3158\n","\t train loss: 13.47 | val loss: 13.54 | train acc: 0.3509 | val acc: 0.3158\n","\t train loss: 9.97 | val loss: 10.00 | train acc: 0.3860 | val acc: 0.3158\n","\t train loss: 6.58 | val loss: 6.72 | train acc: 0.4152 | val acc: 0.4211\n","\t train loss: 3.43 | val loss: 3.74 | train acc: 0.4678 | val acc: 0.4737\n","\t train loss: 1.04 | val loss: 1.16 | train acc: 0.6667 | val acc: 0.6316\n","\t train loss: 0.68 | val loss: 0.36 | train acc: 0.7661 | val acc: 0.8947\n","\t train loss: 0.62 | val loss: 0.34 | train acc: 0.7953 | val acc: 0.8947\n","\t train loss: 0.57 | val loss: 0.47 | train acc: 0.7661 | val acc: 0.6842\n","\t train loss: 0.56 | val loss: 0.44 | train acc: 0.7544 | val acc: 0.7368\n","\t train loss: 0.55 | val loss: 0.37 | train acc: 0.7953 | val acc: 0.7368\n","\t train loss: 0.55 | val loss: 0.37 | train acc: 0.7719 | val acc: 0.7368\n","\t train loss: 0.55 | val loss: 0.40 | train acc: 0.7544 | val acc: 0.7368\n","\t train loss: 0.55 | val loss: 0.39 | train acc: 0.7602 | val acc: 0.7368\n","\t train loss: 0.55 | val loss: 0.38 | train acc: 0.7602 | val acc: 0.7368\n","Start CV fold: 3\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 36.07 | val loss: 24.23 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 32.36 | val loss: 21.77 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 28.66 | val loss: 19.31 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 24.97 | val loss: 16.86 | train acc: 0.3275 | val acc: 0.4211\n","\t train loss: 21.28 | val loss: 14.40 | train acc: 0.3333 | val acc: 0.4211\n","\t train loss: 17.59 | val loss: 11.95 | train acc: 0.3333 | val acc: 0.4211\n","\t train loss: 13.91 | val loss: 9.52 | train acc: 0.3392 | val acc: 0.4211\n","\t train loss: 10.28 | val loss: 7.13 | train acc: 0.3684 | val acc: 0.4737\n","\t train loss: 6.79 | val loss: 4.81 | train acc: 0.4035 | val acc: 0.5263\n","\t train loss: 3.56 | val loss: 2.64 | train acc: 0.4561 | val acc: 0.5263\n","\t train loss: 1.07 | val loss: 0.99 | train acc: 0.6608 | val acc: 0.6316\n","\t train loss: 0.61 | val loss: 1.23 | train acc: 0.7719 | val acc: 0.5789\n","\t train loss: 0.57 | val loss: 1.16 | train acc: 0.8012 | val acc: 0.5789\n","\t train loss: 0.51 | val loss: 0.83 | train acc: 0.7485 | val acc: 0.7895\n","\t train loss: 0.51 | val loss: 0.84 | train acc: 0.7602 | val acc: 0.7895\n","\t train loss: 0.50 | val loss: 0.94 | train acc: 0.8012 | val acc: 0.7368\n","\t train loss: 0.50 | val loss: 0.92 | train acc: 0.8012 | val acc: 0.7368\n","\t train loss: 0.49 | val loss: 0.89 | train acc: 0.7778 | val acc: 0.7895\n","\t train loss: 0.49 | val loss: 0.90 | train acc: 0.7953 | val acc: 0.7368\n","\t train loss: 0.49 | val loss: 0.91 | train acc: 0.8070 | val acc: 0.7368\n","Start CV fold: 4\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 35.02 | val loss: 33.74 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 31.43 | val loss: 30.18 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 27.85 | val loss: 26.63 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 24.28 | val loss: 23.08 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 20.71 | val loss: 19.53 | train acc: 0.3509 | val acc: 0.2632\n","\t train loss: 17.14 | val loss: 15.97 | train acc: 0.3509 | val acc: 0.2632\n","\t train loss: 13.59 | val loss: 12.43 | train acc: 0.3567 | val acc: 0.2632\n","\t train loss: 10.09 | val loss: 8.91 | train acc: 0.3918 | val acc: 0.2632\n","\t train loss: 6.73 | val loss: 5.47 | train acc: 0.4269 | val acc: 0.3158\n","\t train loss: 3.61 | val loss: 2.35 | train acc: 0.4620 | val acc: 0.4737\n","\t train loss: 1.11 | val loss: 0.55 | train acc: 0.6374 | val acc: 0.8947\n","\t train loss: 0.71 | val loss: 0.26 | train acc: 0.7485 | val acc: 0.8421\n","\t train loss: 0.63 | val loss: 0.24 | train acc: 0.7953 | val acc: 0.8947\n","\t train loss: 0.59 | val loss: 0.28 | train acc: 0.7427 | val acc: 0.8947\n","\t train loss: 0.57 | val loss: 0.26 | train acc: 0.7427 | val acc: 0.8947\n","\t train loss: 0.57 | val loss: 0.24 | train acc: 0.7836 | val acc: 0.8947\n","\t train loss: 0.56 | val loss: 0.24 | train acc: 0.7836 | val acc: 0.8947\n","\t train loss: 0.56 | val loss: 0.25 | train acc: 0.7427 | val acc: 0.8947\n","\t train loss: 0.56 | val loss: 0.24 | train acc: 0.7485 | val acc: 0.8947\n","\t train loss: 0.56 | val loss: 0.24 | train acc: 0.7544 | val acc: 0.8947\n","Start CV fold: 5\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 34.96 | val loss: 34.26 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 31.37 | val loss: 30.71 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 27.79 | val loss: 27.17 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 24.22 | val loss: 23.63 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 20.65 | val loss: 20.09 | train acc: 0.3392 | val acc: 0.3684\n","\t train loss: 17.07 | val loss: 16.56 | train acc: 0.3392 | val acc: 0.3684\n","\t train loss: 13.51 | val loss: 13.09 | train acc: 0.3392 | val acc: 0.4211\n","\t train loss: 10.00 | val loss: 9.67 | train acc: 0.3743 | val acc: 0.4211\n","\t train loss: 6.62 | val loss: 6.34 | train acc: 0.4094 | val acc: 0.4737\n","\t train loss: 3.49 | val loss: 3.22 | train acc: 0.4678 | val acc: 0.4737\n","\t train loss: 1.07 | val loss: 0.92 | train acc: 0.6491 | val acc: 0.7368\n","\t train loss: 0.65 | val loss: 0.75 | train acc: 0.7602 | val acc: 0.7368\n","\t train loss: 0.61 | val loss: 0.68 | train acc: 0.7836 | val acc: 0.7368\n","\t train loss: 0.56 | val loss: 0.48 | train acc: 0.7661 | val acc: 0.6316\n","\t train loss: 0.55 | val loss: 0.48 | train acc: 0.7661 | val acc: 0.6842\n","\t train loss: 0.54 | val loss: 0.53 | train acc: 0.8070 | val acc: 0.6842\n","\t train loss: 0.54 | val loss: 0.52 | train acc: 0.8129 | val acc: 0.6842\n","\t train loss: 0.54 | val loss: 0.50 | train acc: 0.7836 | val acc: 0.6316\n","\t train loss: 0.53 | val loss: 0.50 | train acc: 0.8012 | val acc: 0.6316\n","\t train loss: 0.53 | val loss: 0.51 | train acc: 0.8070 | val acc: 0.6842\n","Start CV fold: 6\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 35.26 | val loss: 31.51 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 31.63 | val loss: 28.38 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 28.00 | val loss: 25.25 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 24.38 | val loss: 22.14 | train acc: 0.3099 | val acc: 0.5789\n","\t train loss: 20.77 | val loss: 19.02 | train acc: 0.3158 | val acc: 0.5789\n","\t train loss: 17.15 | val loss: 15.90 | train acc: 0.3158 | val acc: 0.5789\n","\t train loss: 13.55 | val loss: 12.78 | train acc: 0.3216 | val acc: 0.5789\n","\t train loss: 10.00 | val loss: 9.68 | train acc: 0.3567 | val acc: 0.5789\n","\t train loss: 6.59 | val loss: 6.72 | train acc: 0.4035 | val acc: 0.5263\n","\t train loss: 3.43 | val loss: 3.91 | train acc: 0.4561 | val acc: 0.5263\n","\t train loss: 1.01 | val loss: 1.61 | train acc: 0.6550 | val acc: 0.6842\n","\t train loss: 0.61 | val loss: 0.92 | train acc: 0.7895 | val acc: 0.6842\n","\t train loss: 0.57 | val loss: 0.89 | train acc: 0.8012 | val acc: 0.6842\n","\t train loss: 0.51 | val loss: 0.98 | train acc: 0.7719 | val acc: 0.6842\n","\t train loss: 0.50 | val loss: 0.97 | train acc: 0.7602 | val acc: 0.6842\n","\t train loss: 0.49 | val loss: 0.90 | train acc: 0.7953 | val acc: 0.7368\n","\t train loss: 0.49 | val loss: 0.90 | train acc: 0.7953 | val acc: 0.7368\n","\t train loss: 0.49 | val loss: 0.92 | train acc: 0.7602 | val acc: 0.6842\n","\t train loss: 0.49 | val loss: 0.92 | train acc: 0.7661 | val acc: 0.6842\n","\t train loss: 0.49 | val loss: 0.91 | train acc: 0.7661 | val acc: 0.6842\n","Start CV fold: 7\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 34.52 | val loss: 38.22 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 30.98 | val loss: 34.24 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 27.45 | val loss: 30.27 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 23.92 | val loss: 26.31 | train acc: 0.3509 | val acc: 0.2105\n","\t train loss: 20.40 | val loss: 22.34 | train acc: 0.3567 | val acc: 0.2105\n","\t train loss: 16.88 | val loss: 18.37 | train acc: 0.3567 | val acc: 0.2105\n","\t train loss: 13.37 | val loss: 14.42 | train acc: 0.3626 | val acc: 0.2105\n","\t train loss: 9.91 | val loss: 10.56 | train acc: 0.3860 | val acc: 0.3158\n","\t train loss: 6.57 | val loss: 6.84 | train acc: 0.4269 | val acc: 0.3158\n","\t train loss: 3.48 | val loss: 3.37 | train acc: 0.4678 | val acc: 0.4211\n","\t train loss: 1.10 | val loss: 0.72 | train acc: 0.6667 | val acc: 0.5789\n","\t train loss: 0.70 | val loss: 0.30 | train acc: 0.7427 | val acc: 0.9474\n","\t train loss: 0.63 | val loss: 0.26 | train acc: 0.7836 | val acc: 0.9474\n","\t train loss: 0.59 | val loss: 0.25 | train acc: 0.7427 | val acc: 0.8947\n","\t train loss: 0.57 | val loss: 0.24 | train acc: 0.7485 | val acc: 0.8421\n","\t train loss: 0.57 | val loss: 0.23 | train acc: 0.7778 | val acc: 0.9474\n","\t train loss: 0.57 | val loss: 0.23 | train acc: 0.7719 | val acc: 0.9474\n","\t train loss: 0.57 | val loss: 0.23 | train acc: 0.7427 | val acc: 0.8421\n","\t train loss: 0.56 | val loss: 0.23 | train acc: 0.7602 | val acc: 0.8421\n","\t train loss: 0.56 | val loss: 0.23 | train acc: 0.7778 | val acc: 0.8421\n","Start CV fold: 8\n","nb train samples: 171 | nb val samples: 19\n","\t train loss: 33.57 | val loss: 46.75 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 30.12 | val loss: 42.01 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 26.67 | val loss: 37.27 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 23.23 | val loss: 32.54 | train acc: 0.3450 | val acc: 0.2632\n","\t train loss: 19.79 | val loss: 27.81 | train acc: 0.3509 | val acc: 0.2632\n","\t train loss: 16.35 | val loss: 23.08 | train acc: 0.3509 | val acc: 0.2632\n","\t train loss: 12.93 | val loss: 18.35 | train acc: 0.3567 | val acc: 0.2632\n","\t train loss: 9.57 | val loss: 13.66 | train acc: 0.3918 | val acc: 0.2632\n","\t train loss: 6.33 | val loss: 9.12 | train acc: 0.4269 | val acc: 0.3158\n","\t train loss: 3.31 | val loss: 4.96 | train acc: 0.4678 | val acc: 0.4211\n","\t train loss: 1.02 | val loss: 1.55 | train acc: 0.6608 | val acc: 0.6316\n","\t train loss: 0.65 | val loss: 0.42 | train acc: 0.7661 | val acc: 0.8947\n","\t train loss: 0.61 | val loss: 0.40 | train acc: 0.7953 | val acc: 0.8947\n","\t train loss: 0.56 | val loss: 0.60 | train acc: 0.7544 | val acc: 0.8421\n","\t train loss: 0.55 | val loss: 0.56 | train acc: 0.7485 | val acc: 0.8421\n","\t train loss: 0.54 | val loss: 0.46 | train acc: 0.7719 | val acc: 0.8947\n","\t train loss: 0.54 | val loss: 0.47 | train acc: 0.7485 | val acc: 0.9474\n","\t train loss: 0.54 | val loss: 0.51 | train acc: 0.7427 | val acc: 0.8421\n","\t train loss: 0.54 | val loss: 0.50 | train acc: 0.7485 | val acc: 0.8421\n","\t train loss: 0.54 | val loss: 0.49 | train acc: 0.7427 | val acc: 0.8421\n","\t train loss: 34.51 | val loss: 38.28 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 30.96 | val loss: 34.38 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 27.42 | val loss: 30.50 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 23.89 | val loss: 26.62 | train acc: 0.3333 | val acc: 0.3684\n","\t train loss: 20.35 | val loss: 22.74 | train acc: 0.3392 | val acc: 0.3684\n","\t train loss: 16.82 | val loss: 18.85 | train acc: 0.3392 | val acc: 0.3684\n","\t train loss: 13.31 | val loss: 14.97 | train acc: 0.3450 | val acc: 0.3684\n","\t train loss: 9.84 | val loss: 11.12 | train acc: 0.3801 | val acc: 0.3684\n","\t train loss: 6.51 | val loss: 7.45 | train acc: 0.4094 | val acc: 0.4737\n","\t train loss: 3.41 | val loss: 4.07 | train acc: 0.4620 | val acc: 0.4737\n","\t train loss: 1.06 | val loss: 1.17 | train acc: 0.6667 | val acc: 0.5789\n","\t train loss: 0.63 | val loss: 0.86 | train acc: 0.7778 | val acc: 0.6842\n","\t train loss: 0.59 | val loss: 0.79 | train acc: 0.8012 | val acc: 0.6842\n","\t train loss: 0.54 | val loss: 0.59 | train acc: 0.7602 | val acc: 0.6842\n","\t train loss: 0.53 | val loss: 0.59 | train acc: 0.7661 | val acc: 0.6842\n","\t train loss: 0.53 | val loss: 0.64 | train acc: 0.8129 | val acc: 0.6316\n","\t train loss: 0.52 | val loss: 0.63 | train acc: 0.8129 | val acc: 0.6842\n","\t train loss: 0.52 | val loss: 0.61 | train acc: 0.7895 | val acc: 0.5789\n","\t train loss: 0.52 | val loss: 0.61 | train acc: 0.7953 | val acc: 0.6316\n","\t train loss: 0.52 | val loss: 0.62 | train acc: 0.8012 | val acc: 0.6842\n","mean: 0.8578948\n","std: 0.085028924\n","min: 0.7368420958518982\n","q_25: 0.7894737124443054\n","q_50: 0.8684210479259491\n","q_75: 0.9473684430122375\n","max: 0.9473684430122375\n"]}]}]}